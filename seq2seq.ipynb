{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.9 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "fd06fcc8e98ad5cb7ff5f663220dfaef61ff524311afe4101d2fa3d6b76ddf5c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from models import Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = pd.read_csv('fra-eng/fra.txt', names=['src', 'tar', 'lic'], sep='\\t')\n",
    "del lines['lic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = lines[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.tar = lines.tar.apply(lambda x: '\\t'+x+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_eng = tf.keras.preprocessing.text.Tokenizer(num_words=100, char_level=True)\n",
    "tokenizer_fra = tf.keras.preprocessing.text.Tokenizer(num_words=100, char_level=True)\n",
    "tokenizer_eng.fit_on_texts(lines.src)\n",
    "tokenizer_fra.fit_on_texts(lines.tar)\n",
    "word_index_eng = tokenizer_eng.word_index\n",
    "word_index_fra = tokenizer_fra.word_index\n",
    "seq_eng = tokenizer_eng.texts_to_sequences(lines.src)\n",
    "seq_fra = tokenizer_fra.texts_to_sequences(lines.tar)\n",
    "seq_fra_tar = tokenizer_fra.texts_to_sequences(lines.tar.apply(lambda x: x[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_eng_len = max([len(x) for x in seq_eng])\n",
    "max_seq_fra_len = max([len(x) for x in seq_fra])\n",
    "dataset_eng = to_categorical(pad_sequences(seq_eng, maxlen=max_seq_eng_len, padding='post'))\n",
    "dataset_fra = to_categorical(pad_sequences(seq_fra, maxlen=max_seq_fra_len, padding='post'))\n",
    "dataset_fra_tar = to_categorical(pad_sequences(seq_fra_tar, maxlen=max_seq_fra_len, padding='post'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(dataset_eng.shape[2], dataset_fra.shape[2], state_dim=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_eng = dict((i, char) for char, i in word_index_eng.items())\n",
    "index_to_fra = dict((i, char) for char, i in word_index_fra.items())\n",
    "\n",
    "def sampling(model, x):\n",
    "    pred_val = \"\"\n",
    "    _, context_state = model.encode(x)\n",
    "    dec_in = np.zeros((1, 1, dataset_fra.shape[2]))\n",
    "    dec_in[0, 0, word_index_fra['\\t']] = 1.\n",
    "\n",
    "    while True:\n",
    "        y_pred, dec_state_h, dec_state_c = model.decode(dec_in, context_state[0], context_state[1])\n",
    "        target_word_idx = np.argmax(y_pred[0, 0, :])\n",
    "\n",
    "        if index_to_fra[target_word_idx] == '\\n':\n",
    "            break\n",
    "\n",
    "        pred_val += index_to_fra[target_word_idx]\n",
    "\n",
    "        if len(pred_val) == max_seq_fra_len-2:\n",
    "            break\n",
    "\n",
    "        dec_in = np.zeros((1, 1, dataset_fra.shape[2]))\n",
    "        dec_in[0, 0, target_word_idx] = 1.\n",
    "        context_state = [dec_state_h, dec_state_c]\n",
    "    \n",
    "    return pred_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, x, y, y_true):\n",
    "    _, context_state = model.encode(x)\n",
    "    y_pred, _, _ = model.decode(y, context_state[0], context_state[1])\n",
    "    loss = tf.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, x, y, y_true, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(model, x, y, y_true)\n",
    "    gradients = tape.gradient(loss, model.encoder.trainable_variables + model.decoder.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.encoder.trainable_variables + model.decoder.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dataset_eng, dataset_fra, dataset_fra_tar)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eng, fra, fra_tar in train_dataset.take(1):\n",
    "    test_eng = eng[:1]\n",
    "    test_fra = fra[:1]\n",
    "    test_fra_tar = fra_tar[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.RMSprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sample(epoch, test_eng, test_fra):\n",
    "    eng_idxs = np.argmax(test_eng[0,:], axis=1)\n",
    "    eng_sentence = \"\" \n",
    "    for idx in eng_idxs:\n",
    "        if idx == 0:\n",
    "            break\n",
    "        eng = index_to_eng[idx]\n",
    "        eng_sentence += eng\n",
    "\n",
    "    fra_sentence = sampling(model, test_eng)\n",
    "\n",
    "    print(\"{}. {} : {}\".format(epoch, eng_sentence, fra_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, 2):\n",
    "    for train_data in train_dataset:\n",
    "        train_step(model, train_data[0], train_data[1], train_data[2], optimizer)\n",
    "    test_sample(epoch, test_eng, test_fra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('./checkpoints/0610')"
   ]
  }
 ]
}